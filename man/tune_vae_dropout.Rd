% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tune_vae_dropout.R
\name{tune_vae_dropout}
\alias{tune_vae_dropout}
\title{tuning dropout for midae}
\usage{
tune_vae_dropout(
  data,
  dropout.grid = list(input.dropout = c(0.2, 0.5, 0.7), hidden.dropout = c(0.3, 0.6,
    0.8)),
  m = 5,
  epochs = 5,
  batch.size = 32,
  split.ratio = 0.7,
  shuffle = TRUE,
  optimizer = "adamW",
  learning.rate = 1e-04,
  weight.decay = 0,
  momentum = 0,
  encoder.structure = c(128, 64, 32),
  latent.dim = 8,
  decoder.structure = c(32, 64, 128),
  act = "elu",
  init.weight = "xavier.normal",
  scaler = "minmax",
  verbose = TRUE,
  print.every.n = 1,
  save.model = FALSE,
  path = NULL
)
}
\arguments{
\item{data}{A data frame, tibble or data table with missing values.}

\item{dropout.grid}{A list of tuning values for input.dropout and hidden.dropout}

\item{m}{The number of imputed datasets.}

\item{epochs}{The number of training epochs (iterations).}

\item{batch.size}{The size of samples in each batch.}

\item{split.ratio}{The ratio of training data. Default: 0.7.}

\item{shuffle}{Whether or not to shuffle training data. Default: TRUE}

\item{optimizer}{The name of the optimizer. Options are : "adamW" (default), "adam" and "sgd".}

\item{learning.rate}{The learning rate. The default value is 0.001.}

\item{weight.decay}{Weight decay (L2 penalty). The default value is 0.}

\item{momentum}{Parameter for "sgd" optimizer. It is used for accelerating SGD in the relevant direction and dampens oscillations.}

\item{encoder.structure}{A vector indicating the structure of encoder. Default: c(128,64,32)}

\item{latent.dim}{The size of latent layer. The default value is 8.}

\item{decoder.structure}{A vector indicating the structure of decoder. Default: c(32,64,128)}

\item{act}{The name of activation function. Can be: "relu", "elu", "leaky.relu", "tanh", "sigmoid" and "identity".}

\item{init.weight}{Techniques for weights initialization. Can be "xavier.uniform" or "kaiming.uniform".}

\item{scaler}{The name of scaler for transforming numeric features. Can be "standard", "minmax" or "none".}

\item{verbose}{Whether or not to print training loss information. Default: TRUE.}

\item{print.every.n}{If verbose is set to TRUE, print out training loss for every n epochs.}

\item{save.model}{Whether or not to save the imputation model. Default: FALSE.}

\item{path}{The path where the final imputation model will be saved.}

\item{input.dropout}{The dropout probability of the input layer.}

\item{hidden.dropout}{The dropout probability of the hidden layers.}
}
\description{
tuning dropout for midae
}
